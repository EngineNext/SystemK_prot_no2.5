import os
import json
import glob
from pypdf import PdfReader
import google.generativeai as genai
from dotenv import load_dotenv

# --- 設定 ---
# .envファイルからAPIキーを読み込むか、ここに直接書く（非推奨だがテスト用なら可）
# os.environ["GOOGLE_API_KEY"] = "ここにGeminiのAPIキーを入れる"
# .envファイルを、このファイル(generator.py)の「一つ上の階層」から探す命令
env_path = os.path.join(os.path.dirname(__file__), '../.env')
load_dotenv(env_path)

# Geminiの設定
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
model = genai.GenerativeModel('gemini-1.5-pro-latest')

# フォルダ設定
PDF_DIR = os.path.join(os.path.dirname(__file__), 'pdfs')
OUTPUT_FILE = os.path.join(os.path.dirname(__file__), '../js/data.js')

def extract_text_from_pdf(pdf_path):
    print(f"[READ] Reading: {os.path.basename(pdf_path)}...")
    reader = PdfReader(pdf_path)
    text = ""
    # 全ページ読むと長すぎる場合があるので、入試科目がありそうなページ（P10-50など）に絞るのも手
    # 今回は全ページ読む
    for page in reader.pages:
        text += page.extract_text() + "\n"
    return text

def generate_university_data(text):
    print(f"[AI] Analyzing with Gemini...")
    
    prompt = """
    あなたは大学入試のデータアナリストです。
    以下の「募集要項」のテキストデータから、指定されたJSONフォーマットでデータを抽出してください。
    
    【抽出ルール】
    1. 学部・学科ごと、入試方式ごとにデータを分けること。
    2. 科目の配点（weights）は、共通テストの配点比率を抽出すること。
       - 特に記述がなければ標準を「1」とする。
       - 英語のリーディング(R)とリスニング(L)の比率は慎重に判定すること。
    3. 合格最低点やボーダー得点率（borderScoreRate）がテキストにあれば抽出。なければ 0.65 (65%) と推測して入れること。
    
    【出力フォーマット】
    以下のJSON配列形式のみを出力してください。Markdownタグ（```json）は不要です。
    
    [
        {
            "name": "大学名",
            "faculty": "学部名",
            "field": ["science" | "humanities" などの分野タグを2-3個推測],
            "exams": [
                {
                    "type": "入試方式名 (例: 前期, A方式)",
                    "borderScoreRate": 0.75,
                    "desc": "方式の特徴を20文字程度で要約",
                    "weights": {
                        "englishR": 1, "englishL": 1, "math1A": 1, "math2BC": 1,
                        "science1": 1, "science2": 1, "social": 1, "info": 1,
                        "jp_modern": 1, "jp_ancient": 1, "jp_kanbun": 1
                    }
                }
            ]
        }
    ]
    
    【対象テキスト】
    """ + text[:30000] # 文字数制限対策（長すぎるとエラーになるため冒頭3万文字）

    try:
        response = model.generate_content(prompt)
        # JSON部分だけを取り出す簡易的な処理
        json_str = response.text.strip()
        if "```json" in json_str:
            json_str = json_str.split("```json")[1].split("```")[0]
        elif "```" in json_str:
            json_str = json_str.split("```")[1].split("```")[0]
            
        return json.loads(json_str)
    except Exception as e:
        print(f"[ERROR] Error: {e}")
        return []

def save_to_js(universities_data):
    # 既存の静的データ（JS側の設定など）は残しつつ、universities配列だけ書き換える必要があるが
    # 今回は data.js を「universities定義専用」にしたので丸ごと上書きする
    
    # 既存の固定データを定義（消したくない定数など）
    header_content = """// --- Data Definitions ---
// Auto-generated by tools/generator.py

const conversionTables = {
    englishR:  { max: 100, points: [[100, 68.0], [80, 58.7], [60, 49.4], [40, 40.1], [20, 30.8], [0, 20.0]] },
    englishL:  { max: 100, points: [[100, 77.5], [80, 65.0], [60, 52.4], [40, 39.8], [20, 27.2], [0, 20.0]] },
    math1A:    { max: 100, points: [[100, 78.6], [80, 68.7], [60, 58.7], [40, 48.0], [20, 38.0], [0, 28.0]] },
    math2BC:   { max: 100, points: [[100, 68.5], [80, 59.4], [60, 50.3], [40, 41.1], [20, 32.0], [0, 22.0]] },
    japanese:  { max: 200, points: [[200, 79.9], [150, 63.2], [100, 46.6], [80, 39.9], [50, 29.9], [0, 20.0]] },
    jp_modern: { max: 110, points: [[110, 74.6], [90, 62.7], [70, 50.8], [50, 38.9], [30, 27.0], [10, 15.1]] },
    jp_ancient:{ max: 45, points: [[45, 72.0], [35, 62.3], [25, 52.6], [15, 42.9], [5, 30.0]] },
    jp_kanbun: { max: 45, points: [[45, 71.0], [35, 62.8], [25, 54.7], [15, 46.6], [5, 38.0]] },
    science:   { max: 100, points: [[100, 72.0], [80, 64.0], [60, 56.0], [40, 48.0], [20, 40.0], [0, 30.0]] },
    social:    { max: 100, points: [[100, 73.8], [80, 62.9], [60, 52.1], [40, 41.2], [20, 30.0], [0, 20.0]] },
    info:      { max: 100, points: [[100, 81.2], [80, 67.2], [60, 53.2], [40, 39.2], [20, 25.2], [0, 15.0]] }
};

const subjectMaster = {
    englishR: { name: '英語(R)', max: 100, table: 'englishR' },
    englishL: { name: '英語(L)', max: 100, table: 'englishL' },
    math1A:   { name: '数学I・A', max: 100, table: 'math1A' },
    math2BC:  { name: '数学II・B・C', max: 100, table: 'math2BC' },
    jp_modern: { name: '現代文', max: 110, table: 'jp_modern', isSub: true },
    jp_ancient:{ name: '古文', max: 45, table: 'jp_ancient', isSub: true },
    jp_kanbun: { name: '漢文', max: 45, table: 'jp_kanbun', isSub: true },
    info:     { name: '情報', max: 100, table: 'info' },
    science1: { name: '理科①', max: 100, table: 'science' },
    science2: { name: '理科②', max: 100, table: 'science' },
    social:   { name: '地歴公民', max: 100, table: 'social' },
    social1:  { name: '地歴公民①', max: 100, table: 'social' },
    social2:  { name: '地歴公民②', max: 100, table: 'social' },
    scienceBasic: { name: '理科基礎', max: 100, table: 'science' }
};
"""

    # 自動生成されたデータを追記
    js_content = header_content + "\nconst universities = " + json.dumps(universities_data, indent=4, ensure_ascii=False) + ";"
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(js_content)
    print(f"[DONE] Updated {OUTPUT_FILE} with {len(universities_data)} universities!")

def main():
    all_universities = []
    
    # pdfsフォルダ内のPDFを全走査
    pdf_files = glob.glob(os.path.join(PDF_DIR, "*.pdf"))
    
    if not pdf_files:
        print("[WARN] No PDF files found in tools/pdfs/")
        return

    for pdf_path in pdf_files:
        text = extract_text_from_pdf(pdf_path)
        if text:
            data = generate_university_data(text)
            all_universities.extend(data)
    
    # JSファイルに書き込み
    save_to_js(all_universities)

if __name__ == "__main__":
    main()
